{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcGIS Online Dashboard - Hosted Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard script for ArcGIS Hosted Notebooks\n",
    "# Based on University of Michigan Dashboard project by Peter Knoop and Abbey Roelofs\n",
    "# https://community.esri.com/t5/arcgis-dashboards-blog/gis-for-everyone-and-how-to-build-your-own-arcgis/ba-p/903706\n",
    "# Removed control flow statements; adapted for ArcGIS Hosted Notebook environment.\n",
    "# -David Merten-Jones, 2024/09/04\n",
    "\n",
    "import arcgis\n",
    "from arcgis import GIS, features\n",
    "\n",
    "import contextlib\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import codecs\n",
    "import math\n",
    "import csv\n",
    "import yaml\n",
    "\n",
    "#Locating external .py module on ArcGIS Online proved difficult.\n",
    "#from home.dashboard.rest_api_functions import *\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTIONS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# These functions would normally go into an external .py file; ArcGIS\n",
    "#Hosted Jupyter Notebooks are not set up to import functions from .py files,\n",
    "#so they are all included here instead.\n",
    "\n",
    "def submit_request(request, response_type='json'):\n",
    "    \n",
    "    #\"request\" is an instance of \"Request()\" from urllib\n",
    "    #\"response_type\" can be 'csv' or 'json'\n",
    "    \n",
    "    #used in `get_token()`, `history()`, and `users()` functions\n",
    "    \n",
    "    #5/13/2024: Added .csv functionality; see also get_token() and history() - DMJ\n",
    "    #Returns the response from an HTTP request to the ArcGIS REST API\n",
    "    with contextlib.closing(urlopen(request)) as response:\n",
    "        if response_type == 'json':\n",
    "            job_info = json.load(response)\n",
    "        elif response_type == 'csv':\n",
    "            #job_info = response.read()\n",
    "            #json.dumps(list(csv.DictReader(open(response))))\n",
    "            job_info = []\n",
    "            for line in csv.DictReader(codecs.iterdecode(response, 'utf-8')):\n",
    "                job_info.append(line)\n",
    "        \n",
    "        return job_info\n",
    "    \n",
    "def get_token(portal_url, username, password, response_type='json'):\n",
    "    # Returns an authentication token for use with REST API calls.\n",
    "\n",
    "    params = {\"username\": username,\n",
    "              \"password\": password,\n",
    "              \"referer\": portal_url,\n",
    "              \"f\": response_type}\n",
    "\n",
    "    token_url = \"{}/sharing/generateToken\".format(portal_url)\n",
    "    request = Request(token_url, urlencode(params).encode(\"utf-8\"))\n",
    "    token_response = submit_request(request, response_type)\n",
    "    if \"token\" in token_response:\n",
    "        token = token_response.get(\"token\")\n",
    "        return token\n",
    "    else:\n",
    "        # Request for token must be made through HTTPS.\n",
    "        if \"error\" in token_response:\n",
    "            error_message = token_response.get(\"error\", {}).get(\"message\")\n",
    "            if \"This request needs to be made over https.\" in error_message:\n",
    "                token_url = token_url.replace(\"http://\", \"https://\")\n",
    "                \n",
    "                #`get_token()` calls itself recursively if it doesn'\n",
    "                token = get_token(token_url, username, password)\n",
    "                return token\n",
    "            else:\n",
    "                raise Exception(\"Portal error: {} \".format(error_message))\n",
    "            \n",
    "def history(history_url, params):\n",
    "    # Wrapper function for REST API history method.\n",
    "    #5/13/2024: Added .csv functionality; see also get_token() and submit_request() - DMJ\n",
    "    #params['f'] specifies the format. Pass either 'csv' or 'json'\n",
    "    \n",
    "    # Used in `get_actions()` function\n",
    "\n",
    "    request = Request(history_url, urlencode(params).encode(\"utf-8\"))\n",
    "    response = submit_request(request, params['f'])\n",
    "    return response\n",
    "\n",
    "def users(users_url, params):\n",
    "    # Wrapper function for REST API history method.\n",
    "    \n",
    "    # Used in `get_users()` function\n",
    "    \n",
    "    request = Request(users_url, urlencode(params).encode(\"utf-8\"))\n",
    "    response = submit_request(request, params['f'])\n",
    "    return response\n",
    "\n",
    "def get_actions(params, portal_url, username, password, extended_url, is_update=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    get_actions() is a function for retrieving actions\n",
    "    (such as user logins or item edits) from the ArcGIS\n",
    "    REST API.\n",
    "    \n",
    "    parameters are passed in this form:\n",
    "    \n",
    "    params = {\n",
    "        \"num\": 10000,\n",
    "        \"fromDate\": fromDate,\n",
    "        \"toDate\": toDate,\n",
    "        \"sortOrder\": \"asc\",\n",
    "        \"all\": True,\n",
    "        \"types\": \"u\",\n",
    "        \"actions\": \"login\",\n",
    "        \"actors:\": \"*\",\n",
    "        \"f\": \"csv\"\n",
    "    }\n",
    "    \n",
    "    get_update() calls history()\n",
    "    history() calls submit_response() with f='csv'\n",
    "    get_token() calls submit_request()\n",
    "    \n",
    "    See documentation here:\n",
    "    https://developers.arcgis.com/rest/users-groups-and-items/portal-history.htm\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    actions = []\n",
    "    total_retrieved = 0\n",
    "\n",
    "    token = get_token(portal_url, username, password)\n",
    "    params['token'] = token\n",
    "\n",
    "    #Maximum value for num is 100 for json output, 10000 for csv\n",
    "    #The actions fieldname does not always behave as described in\n",
    "    #the REST API documentation\n",
    "\n",
    "    #\"num_actions\" will be set to the output length at the end of\n",
    "    #every iteration through this loop. If the number of returned\n",
    "    #actions is less than 10000 (i.e.: when the actions returned are\n",
    "    #the remainder after dividing the total actions by 10000,\n",
    "    #the loop will end\n",
    "    \n",
    "    num_actions = 10000\n",
    "    while num_actions == 10000:\n",
    "\n",
    "        #Get history, record how many entries were returned\n",
    "        h = history(extended_url, params)\n",
    "        temp_h_len = len(h)\n",
    "\n",
    "        #Since the updated fromDate is pulled from the last retrieved\n",
    "        #record, the next one retrieved will be a duplicate\n",
    "        if is_update == True:\n",
    "            h.pop(0)\n",
    "        \n",
    "        #Extend actions list in chunks of 10000\n",
    "        actions.extend(h)\n",
    "\n",
    "        #Set new fromDate based on timestamp of last retrieved record.\n",
    "        params['fromDate'] = h[-1]['created']\n",
    "        \n",
    "        #Keep track of progress\n",
    "        total_retrieved += len(h)\n",
    "        print('Total Logins Retrieved: {}'.format(total_retrieved))\n",
    "\n",
    "        #After the first pull, set \"is_update\" to True and discard the\n",
    "        #first result (which has a duplicate timestamp == 'fromDate')\n",
    "        is_update = True\n",
    "        \n",
    "        #Move number of actions out of temporary variable into num_actions\n",
    "        num_actions = temp_h_len\n",
    "        \n",
    "    for action in actions:\n",
    "        action['created'] = int(action['created'])\n",
    "    \n",
    "    return actions\n",
    "\n",
    "\n",
    "def time_segments(start, years=0, months=0, weeks=0, days=0, hours=0):\n",
    "    # Generate date ranges based on years, months, weeks, days, or hours \n",
    "    # Create list to hold weekly ranges.\n",
    "    segments = []\n",
    "\n",
    "    # Use current time from a [segment] ago as the cut-off for generating time ranges.\n",
    "    now = datetime.now() \n",
    "    \n",
    "    print(now)\n",
    "\n",
    "    timestamp = start\n",
    "    \n",
    "    # Create the list of periodic time ranges.\n",
    "    while( timestamp <= now ):\n",
    "        date_range = {}\n",
    "        date_range['fromDate'] = timestamp\n",
    "        \n",
    "        timestamp += relativedelta(\n",
    "            years=years*-1, months=months*-1, weeks=weeks*-1,\n",
    "            days=days*-1, hours=hours*-1\n",
    "        )\n",
    "        \n",
    "        date_range['toDate'] = timestamp\n",
    "        if date_range['toDate'] <= now:\n",
    "            segments.append(date_range)\n",
    "\n",
    "    # Convert the list to a Pandas DataFrame to see what we have.\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def actions_per_segment(actions, segments):\n",
    "    # Gather login action data per time segment.\n",
    "\n",
    "    # Accumulate list of results for each time segment.\n",
    "    actions_per_period = []\n",
    "\n",
    "    # Loop through date ranges: time segment.\n",
    "    for date_range in segments:\n",
    "\n",
    "        # Convert time range boundaries to int to match the data type for created.\n",
    "        fromDate = int(date_range['fromDate'].timestamp()*1000)\n",
    "        toDate = int(date_range['toDate'].timestamp()*1000)\n",
    "\n",
    "        # Count total logins for the current time segment.\n",
    "        period_actions = [i for i in actions if i['created'] >= fromDate and i['created'] < toDate]\n",
    "        total = len(period_actions)\n",
    "\n",
    "        # Count unique logins for the current time segment.\n",
    "        period_unique_actions = list(set([d['actor'] for d in period_actions]))\n",
    "        unique = len(period_unique_actions)\n",
    "\n",
    "        # Append results to running list.\n",
    "        results = {\n",
    "            'fromDate': date_range['fromDate'],\n",
    "            'toDate': date_range['toDate'],\n",
    "            'total': total,\n",
    "            'unique': unique\n",
    "        }\n",
    "        actions_per_period.append(results)\n",
    "\n",
    "    return actions_per_period\n",
    "\n",
    "def to_rest_params(per_period_logins):\n",
    "    \"\"\"Format the logins per period so it is REST-readable\n",
    "    and may be added to the Table.\"\"\"\n",
    "    per_period = []\n",
    "\n",
    "    # Loop thru all the weeks of data.\n",
    "    for unit in per_period_logins:\n",
    "        per_period.append({\n",
    "            'attributes': {\n",
    "                'From_Date': unit['fromDate'],\n",
    "                'To_Date': unit['toDate'],\n",
    "                'Total_Logins': unit['total'],\n",
    "                'Unique_Logins': unit['unique']\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return per_period\n",
    "\n",
    "def user_to_rest(records):\n",
    "    adds = []\n",
    "    for entry in records:\n",
    "        adds.append({'attributes':entry})\n",
    "        \n",
    "    return adds\n",
    "\n",
    "def get_users(params, portal_url, username, password, users_url):\n",
    "    \"\"\"\n",
    "    Expected params like:\n",
    "    \n",
    "    params = {\n",
    "        \"num\": 100,              # Max value permitted for num is 100.\n",
    "        \"start\": 1,\n",
    "        \"sortField\": 'username',\n",
    "        \"sortOrder\": \"asc\",\n",
    "        \"f\": \"json\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    users_list = []\n",
    "    users_retrieved = 0\n",
    "\n",
    "    token = get_token(portal_url, username, password)\n",
    "\n",
    "    params['token'] = token\n",
    "\n",
    "    response_length = 100\n",
    "    while response_length == 100:\n",
    "\n",
    "        u = users(users_url, params)\n",
    "\n",
    "        users_list.extend(u['users'])\n",
    "\n",
    "        params['start'] += len(u['users'])\n",
    "        users_retrieved += len(u['users'])\n",
    "        print(\"Users retrieved: {}\".format(users_retrieved))\n",
    "\n",
    "        response_length = len(u['users'])\n",
    "        \n",
    "    return users_list\n",
    "\n",
    "def stamp_to_time(x):\n",
    "    #Convert from UNIX epoch timestamp to YYYY-MM-DD HH:MM:SS\n",
    "    return datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def get_domain(x):\n",
    "    #Separate the domain from the rest of the email address\n",
    "    #Return the domain as a string\n",
    "    result = ''\n",
    "    try:\n",
    "        result = x.split('@')[1].lower().replace('_claremont','')\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "def prioritize_email(a, b):\n",
    "    #\n",
    "    #\n",
    "    if a == '':\n",
    "        aff = b\n",
    "    else:\n",
    "        aff = a\n",
    "\n",
    "    return aff\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCRIPT ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD CREDENTIALS FROM LOCAL YAML FILE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open configuration yaml file\n",
    "with open('/arcgis/home/dashboard/dash_config.yml','r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Provide your ArcGIS Online organization URL,\n",
    "# e.g.: https://umich.maps.arcgis.com).\n",
    "portal_url = config.get('portal_url')\n",
    "\n",
    "# Provide username of an account with the built-in Administrator role,\n",
    "# which is required for access to historical data through the REST API.\n",
    "username = config.get('username')\n",
    "password = config.get('password')\n",
    "\n",
    "# Connect to your GIS.\n",
    "try:\n",
    "    gis = GIS(portal_url, username, password)\n",
    "\n",
    "    print( 'Login successful:')\n",
    "    print( '    server: ' + gis.properties.name )\n",
    "    print( '    user: ' + gis.properties.user.username )\n",
    "    print( '    role: ' + gis.properties.user.role )\n",
    "\n",
    "except:\n",
    "    print(\"Login failed!\")\n",
    "\n",
    "# Define REST endpoint for the history and users method of your ArcGIS Online\n",
    "# instance (e.g., \n",
    "# https://umich.maps.arcgis.com/sharing/rest/portals/4ezfu5dIwH83BUNL/history\n",
    "# and /users).\n",
    "history_url = portal_url + '/sharing/rest/portals/' + gis.properties.id + '/history'\n",
    "users_url = portal_url + '/sharing/rest/portals/' + gis.properties.id + '/users'\n",
    "\n",
    "#### CONNECT TO DASHBOARD AND TABLES VIA AUTOMATED ADMIN LOGIN ####\n",
    "\n",
    "# Get the Dashboard.\n",
    "dashboard_item = gis.content.get(config['dashboard_item'])\n",
    "print('Dashboard Retrieved')\n",
    "\n",
    "# Get the View of the usage data Feature Service,\n",
    "# from which the Dashboard widgets draw their data.\n",
    "usage_view_item = gis.content.get(config['usage_view_item'])\n",
    "print('Feature Service Retrieved')\n",
    "\n",
    "# Get the Feature Service in which the usage data will be stored.\n",
    "usage_item = gis.content.get(config['usage_item'])\n",
    "print('Feature Storage Retrieved')\n",
    "\n",
    "usage_flc = features.FeatureLayerCollection.fromitem(usage_item)\n",
    "\n",
    "# Determine how many users of each type are currently registered on your instance.\n",
    "user_types_df = gis.users.counts(type='user_type')\n",
    "\n",
    "# Sum the user_type counts to determine the total number of currently\n",
    "# registered users.\n",
    "current_user_count = user_types_df['count'].sum()\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INITIALIZE AND POPULATE TABLES ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USER COUNT + WEEKLY + MONTHLY ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any existing data; this Table should only have the one, current record.\n",
    "usage_flc.tables[0].manager.truncate()\n",
    "\n",
    "# Determine how many users of each type are currently registered on your instance.\n",
    "user_types_df = gis.users.counts(type='user_type')\n",
    "print(user_types_df)\n",
    "\n",
    "# Sum the user_type counts to determine the total number of currently registered users.\n",
    "current_user_count = user_types_df['count'].sum()\n",
    "\n",
    "# Create a dictionary of the data to be added to the Table.\n",
    "\n",
    "\"\"\"The edit_features method expects the data to be added is in the form of a\n",
    "python dictionary with a key named 'attributes'. The value of the 'attributes'\n",
    "key is another dictionary containing the Zfield names and values (in this case,\n",
    "Timestamp and Users, where Users is the count of users.)\"\"\"\n",
    "\n",
    "users_dict = {\n",
    "    'attributes': {\n",
    "        'Timestamp': datetime.now(timezone.utc),\n",
    "        'Users': current_user_count\n",
    "    }\n",
    "}\n",
    "\n",
    "# Skip ahead for upload to table\n",
    "\n",
    "################################################################################\n",
    "\n",
    "if config.get('init') == True:\n",
    "    \n",
    "    print(\"Initialization Mode\")\n",
    "   \n",
    "    #### CLEAR EXISTING DATA FROM TABLES ####\n",
    "    \n",
    "    usage_flc.tables[1].manager.truncate()\n",
    "    usage_flc.tables[2].manager.truncate()\n",
    "    usage_flc.tables[3].manager.truncate()\n",
    "    print('Feature Layers Truncated')\n",
    "\n",
    "    #### WEEKLY AND MONTHLY LOGINS - QUERY ARCGIS ONLINE REST API ####\n",
    "    \n",
    "    fromDate = datetime.timestamp(\n",
    "        datetime(2018, 12, 2).replace(tzinfo=timezone.utc)\n",
    "    )\n",
    "    \n",
    "    toDate = datetime.timestamp(\n",
    "        datetime.today().replace(tzinfo=timezone.utc)\n",
    "    )\n",
    "    \n",
    "    params = {\n",
    "        \"num\": 10000,\n",
    "        \"fromDate\": fromDate,\n",
    "        \"toDate\": toDate,\n",
    "        \"sortOrder\": \"asc\",\n",
    "        \"all\": True,\n",
    "        \"types\": \"u\",\n",
    "        \"actions\": \"login\",\n",
    "        \"actors:\": \"*\",\n",
    "        \"f\": \"csv\"\n",
    "    }\n",
    "    \n",
    "    #This takes a few minutes to run:\n",
    "    print('Retrieving User Login Events')\n",
    "    user_logins = get_actions(\n",
    "        params, portal_url, username, password, history_url\n",
    "    )\n",
    "    \n",
    "    #Get time of update\n",
    "    current_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    #populate empty DataFrame with retrieved user login data\n",
    "    df_logins = pd.DataFrame(user_logins)\n",
    "    df_logins['Updated'] = [current_time for i in range(len(df_logins))]\n",
    "    \n",
    "    #Save out logins DataFrame as a parquet file (only on initial run):\n",
    "    df_logins.to_parquet('/arcgis/home/dashboard/user_logins.parquet')\n",
    "    print('DataFrame Saved to Parquet File')\n",
    "        \n",
    "    #### GENERATE WEEKLY/MONTHLY ACTIVITY GRAPHS ####\n",
    "    \n",
    "    #### WEEKLY ####\n",
    "    \n",
    "    # Set starting point for the weekly ranges to Sunday prior to when login\n",
    "    #actions began to be tracked in ArcGIS Online.\n",
    "    timestamp = datetime(2018, 12, 2)\n",
    "    weekly_segments = time_segments(timestamp, weeks=-1)\n",
    "    weekly_logins = actions_per_segment(user_logins, weekly_segments)\n",
    "    \n",
    "    # Create a dictionary of the data to be added to the Table.\n",
    "    weekly = to_rest_params(weekly_logins)\n",
    "\n",
    "    # Add the data to the Table.\n",
    "    # (The Weekly Logins Table has an id of 3.)\n",
    "    edit_result3 = usage_flc.tables[3].edit_features(adds = weekly)\n",
    "    print('Unique Logins by Week Updated')\n",
    "    \n",
    "    #### MONTHLY ####\n",
    "    \n",
    "    # Set starting point for the weekly ranges to Sunday prior to when login\n",
    "    #actions began to be tracked in ArcGIS Online.\n",
    "    timestamp = datetime(2018, 12, 2)\n",
    "    monthly_segments = time_segments(timestamp, months=-1)\n",
    "    monthly_logins = actions_per_segment(user_logins, monthly_segments)\n",
    "    \n",
    "    # Create a dictionary of the data to be added to the Table.\n",
    "    monthly = to_rest_params(monthly_logins)\n",
    "\n",
    "    # Add the data to the Table.\n",
    "    # (The Weekly Logins Table has an id of 2.)\n",
    "    edit_result2 = usage_flc.tables[2].edit_features(adds = monthly)\n",
    "    print('Unique Logins by Month Updated')\n",
    "    \n",
    "    #### STORE LAST RECORDED LOGIN IN YAML FILE ####\n",
    "    \n",
    "    #update config yaml file with last login retrieved\n",
    "    config['last_login'] = int(df_logins['created'].max())\n",
    "    print(config['last_login'])\n",
    "    config['init'] = False\n",
    "    with open('/arcgis/home/dashboard/dash_config.yml', 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "        \n",
    "    print('Initialization Finished')\n",
    "    print('Update Mode Enabled')\n",
    "    \n",
    "################################################################################    \n",
    "    \n",
    "else:\n",
    "    print('Update Mode')\n",
    "    \n",
    "    #### UPDATE TABLES ####\n",
    "    \n",
    "    #### UPDATE LOGINS ####\n",
    "\n",
    "    fromDate = config['last_login']\n",
    "    toDate = datetime.timestamp(datetime.today().replace(tzinfo=timezone.utc))\n",
    "    \n",
    "    params = {\n",
    "        \"num\": 10000,\n",
    "        \"fromDate\": fromDate,\n",
    "        \"toDate\": toDate,\n",
    "        \"sortOrder\": \"asc\",\n",
    "        \"all\": True,\n",
    "        \"types\": \"u\",\n",
    "        \"actions\": \"login\",\n",
    "        \"actors:\": \"*\",\n",
    "        \"f\": \"csv\"\n",
    "    }\n",
    "    \n",
    "    user_logins_update = get_actions(\n",
    "        params, portal_url, username, password, history_url, is_update=True\n",
    "    )\n",
    "    \n",
    "    #Populate empty DataFrame with retrieved user login data\n",
    "    df_update = pd.DataFrame(user_logins_update)\n",
    "    current_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df_update['Updated'] = [current_time for i in range(len(df_update))]\n",
    "    \n",
    "    #Load in previously-saved data from parquet file\n",
    "    df_logins = pd.read_parquet('/arcgis/home/dashboard/user_logins.parquet')\n",
    "    \n",
    "    #Append the newly-retrieved user login data to the previously-saved data\n",
    "    df_total = pd.concat([df_logins, df_update])\n",
    "    \n",
    "    #Reset the index of the new DataFrame\n",
    "    df_total.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #Export the combined data to parquet format, replacing the old parquet file\n",
    "    df_total.to_parquet('/arcgis/home/dashboard/user_logins.parquet')\n",
    "    print('Parquet File Updated')\n",
    "    \n",
    "    #### STORE LAST RECORDED LOGIN IN YAML FILE ####\n",
    "    \n",
    "    config['last_login'] = int(df_update['created'].max())\n",
    "    print(config['last_login'])\n",
    "    \n",
    "\n",
    "################################################################################\n",
    "        \n",
    "    #### WEEKLY ####\n",
    "    \n",
    "\n",
    "    # Clear existing data from table\n",
    "    usage_flc.tables[3].manager.truncate()\n",
    "\n",
    "    login_dictionary = df_total.to_dict('records')\n",
    "\n",
    "    # Set starting point for the weekly ranges to Monday prior to when\n",
    "    # login actions began to be tracked in ArcGIS Online.\n",
    "    timestamp = datetime(2018, 12, 2)\n",
    "    weekly_segments = time_segments(timestamp, weeks=-1)\n",
    "    weekly_logins = actions_per_segment(\n",
    "        login_dictionary, weekly_segments\n",
    "    )\n",
    "\n",
    "    # Create a dictionary of the data to be added to the Table.\n",
    "    weekly = to_rest_params(weekly_logins)\n",
    "\n",
    "    # Add the data to the Table.\n",
    "    # (The Weekly Logins Table has an id of 3.)\n",
    "    edit_result3 = usage_flc.tables[3].edit_features(adds = weekly)\n",
    "    print('Unique Logins by Week Updated')\n",
    "\n",
    "################################################################################\n",
    "    \n",
    "    #### MONTHLY ####\n",
    "    \n",
    "    # Clear existing data from table\n",
    "    usage_flc.tables[2].manager.truncate()\n",
    "\n",
    "    login_dictionary = df_total.to_dict('records')\n",
    "\n",
    "    # Set starting point for the monthly ranges to 1 month prior to when login\n",
    "    #actions began to be tracked in ArcGIS Online.\n",
    "    timestamp = datetime(2018, 12, 2)\n",
    "    monthly_segments = time_segments(timestamp, months=-1)\n",
    "    monthly_logins = actions_per_segment(\n",
    "        login_dictionary, monthly_segments\n",
    "    )\n",
    "\n",
    "    # Create a dictionary of the data to be added to the Table.\n",
    "    monthly = to_rest_params(monthly_logins)\n",
    "\n",
    "    # Add the data to the Table.\n",
    "    # (The Weekly Logins Table has an id of 2.)\n",
    "    edit_result2 = usage_flc.tables[2].edit_features(adds = monthly)\n",
    "    print('Unique Logins by Month Updated')\n",
    "\n",
    "################################################################################    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USERS BY CAMPUS ####\n",
    "\n",
    "This section is specific to the Claremont Colleges, which are comprised by five undergraduate liberal arts colleges and two graduate-level institutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {\n",
    "    \"num\": 100,              # Max value permitted for num is 100.\n",
    "    \"start\": 1,\n",
    "    \"sortField\": 'username',\n",
    "    \"sortOrder\": \"asc\",\n",
    "    'f':'json'\n",
    "}\n",
    "\n",
    "print('Retrieving Users')\n",
    "users_list = get_users(\n",
    "    params2, portal_url, username, password, users_url\n",
    ")\n",
    "\n",
    "print(len(users_list))\n",
    "\n",
    "df_users = pd.DataFrame(users_list)\n",
    "\n",
    "#Replace erroneous logins with \"0\"\n",
    "df_users['lastLogin'] = df_users['lastLogin'].apply(\n",
    "    lambda x: 0 if x == -1 else x\n",
    ")\n",
    "\n",
    "#Convert users' last login timestamp more easily readable object\n",
    "df_users['lastLoginReadable'] = df_users['lastLogin'].apply(\n",
    "    lambda x: stamp_to_time(x)\n",
    ")\n",
    "\n",
    "#Extract users' school email domains\n",
    "df_users['domain_user'] = df_users['username'].apply(get_domain)\n",
    "df_users['domain_email'] = df_users['email'].apply(get_domain)\n",
    "domain_list = df_users['domain_user'].unique().tolist()\n",
    "domain_list.extend(df_users['domain_email'].unique().tolist())\n",
    "domain_list = list(set(domain_list))\n",
    "\n",
    "#Dictionary for converting domains to school affiliation\n",
    "affiliation = {\n",
    "    '':'',\n",
    "    'apu.edu':'Other',\n",
    "    'blakeschool.org':'Other',\n",
    "    'calbg.org':'Other',\n",
    "    'cgu.edu':'Claremont Graduate University',\n",
    "    'claremont.edu':'Other',\n",
    "    'claremontmckenna.edu':'Claremont McKenna College',\n",
    "    'clemson.edu':'Other',\n",
    "    'cmc.edu':'Claremont McKenna College',\n",
    "    'cpp.edu':'Other',\n",
    "    'csuci.edu':'Other',\n",
    "    'cuc.claremont.edu':'Other',\n",
    "    'esri.com':'Other',\n",
    "    'g.hmc.edu':'Harvey Mudd College',\n",
    "    'gmail.com':'Other',\n",
    "    'hamilton.edu':'Other',\n",
    "    'hive.claremont.edu':'Pomona College',\n",
    "    'hmc.edu':'Harvey Mudd College',\n",
    "    'icloud.com':'Other',\n",
    "    'kecksci.claremont.edu':'Keck Graduate Institute',\n",
    "    'kgi.edu':'Keck Graduate Institute',\n",
    "    'leeds.ac.uk':'Other',\n",
    "    'live.cm':'Other',\n",
    "    'live.com':'Other',\n",
    "    'mac.com':'Other',\n",
    "    'macalester.edu':'Other',\n",
    "    'minerva.kgi.edu':'Keck Graduate Institute',\n",
    "    'my.csun.edu':'Other',\n",
    "    'mymail.pomona.edu':'Pomona College',\n",
    "    'outlook.com':'Other',\n",
    "    'pitzer.edu':'Pitzer College',\n",
    "    'pom':'Pomona College',\n",
    "    'pomona.edu':'Pomona College',\n",
    "    'redlands.edu':'Other',\n",
    "    'rsabg.org':'Other',\n",
    "    'sas.upenn.edu':'Other',\n",
    "    'scrippscollege.edu':'Scripps College',\n",
    "    'student.pitzer.edu':'Pitzer College',\n",
    "    'students.claremontmckenna.edu':'Claremont McKenna College',\n",
    "    'students.pitzer.edu':'Pitzer College',\n",
    "    'yahoo.com':'Other'\n",
    "}\n",
    "\n",
    "#Apply lambda functions to get school affiliation\n",
    "df_users['affiliation_user'] = df_users['domain_user'].apply(\n",
    "    lambda x: affiliation.get(x, 'Other')\n",
    ")\n",
    "\n",
    "df_users['affiliation_email'] = df_users['domain_email'].apply(\n",
    "    lambda x: affiliation.get(x, 'Other')\n",
    ")\n",
    "\n",
    "df_users['affiliation'] = df_users.apply(\n",
    "    lambda x: prioritize_email(x['affiliation_email'],\n",
    "    x['affiliation_user']), axis=1\n",
    ")\n",
    "\n",
    "affiliation_list = ['Pitzer College', 'Pomona College',\n",
    "    'Claremont McKenna College', 'Scripps College',\n",
    "    'Claremont Graduate University', 'Harvey Mudd College', 'Other',\n",
    "    'Keck Graduate Institute']\n",
    "\n",
    "#Insert \"Other\" for users with no email domain\n",
    "df_users['affiliation'] = df_users['affiliation_email'].apply(\n",
    "    lambda x: 'Other' if x not in affiliation_list else x\n",
    ")\n",
    "\n",
    "#Dataframe used to populate donut chart\n",
    "affiliation_by_email = df_users['affiliation'].value_counts()\n",
    "df_affil = pd.DataFrame(affiliation_by_email).reset_index()\n",
    "\n",
    "# Get the Dashboard.\n",
    "users_by_campus = gis.content.get(config['users_by_campus'])\n",
    "\n",
    "df_affil = df_affil.rename(columns={'count':'count_'})\n",
    "\n",
    "adds = df_affil.to_dict(\"records\")\n",
    "\n",
    "adds = user_to_rest(adds)\n",
    "\n",
    "#Delete existing data in table\n",
    "users_by_campus.tables[0].manager.truncate()\n",
    "\n",
    "#Upload new table\n",
    "users_by_campus.tables[0].edit_features(adds)\n",
    "users_by_campus\n",
    "\n",
    "print('Users by Campus Updated')\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD USER COUNT (USAGE_FLC.TABLES[0]) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful:\n",
      "    server: The Claremont Colleges Library\n",
      "    user: TCCLDashboardAdmin\n",
      "    role: org_admin\n",
      "Dashboard Retrieved\n",
      "Feature Service Retrieved\n",
      "Feature Storage Retrieved\n",
      "                    key  count\n",
      "0  GISProfessionalAdvUT   2942\n",
      "Update Mode\n",
      "Total Logins Retrieved: 233\n",
      "Parquet File Updated\n",
      "1725561136010\n",
      "2024-09-05 18:32:29.254437\n",
      "Unique Logins by Week Updated\n",
      "2024-09-05 18:33:16.411491\n",
      "Unique Logins by Month Updated\n",
      "Retrieving Users\n",
      "Users retrieved: 100\n",
      "Users retrieved: 200\n",
      "Users retrieved: 300\n",
      "Users retrieved: 400\n",
      "Users retrieved: 500\n",
      "Users retrieved: 600\n",
      "Users retrieved: 700\n",
      "Users retrieved: 800\n",
      "Users retrieved: 900\n",
      "Users retrieved: 1000\n",
      "Users retrieved: 1100\n",
      "Users retrieved: 1200\n",
      "Users retrieved: 1300\n",
      "Users retrieved: 1400\n",
      "Users retrieved: 1500\n",
      "Users retrieved: 1600\n",
      "Users retrieved: 1700\n",
      "Users retrieved: 1800\n",
      "Users retrieved: 1900\n",
      "Users retrieved: 2000\n",
      "Users retrieved: 2100\n",
      "Users retrieved: 2200\n",
      "Users retrieved: 2300\n",
      "Users retrieved: 2400\n",
      "Users retrieved: 2500\n",
      "Users retrieved: 2600\n",
      "Users retrieved: 2700\n",
      "Users retrieved: 2800\n",
      "Users retrieved: 2900\n",
      "Users retrieved: 2942\n",
      "2942\n",
      "Users by Campus Updated\n",
      "Registered Users Updated\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Add the data to the Table.\n",
    "# (The Currently Registered Users Table has an id of 0.)\n",
    "edit_result = usage_flc.tables[0].edit_features(adds = [users_dict])\n",
    "print('Registered Users Updated')\n",
    "\n",
    "#### UPDATE YAML FILE ####\n",
    "\n",
    "config['current_user_count'] = int(current_user_count)\n",
    "\n",
    "with open('/arcgis/home/dashboard/dash_config.yml', 'w') as file:\n",
    "    yaml.dump(config, file, default_flow_style=False)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "10.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
